# ğŸ“„ PDF Q&A Tool

A lightweight, local-first PDF Question Answering app powered by **FastAPI**, **Streamlit**, **FAISS**, and **LLMs** like **LLaMA3** via Ollama. It lets you upload PDFs, ask questions, and get answers grounded in the document content.

---

## ğŸ”§ Features

- ğŸ“š PDF text extraction via `PyMuPDF`
- ğŸ’¡ Question Answering using LLaMA3 (via Ollama API)
- ğŸ§  Embedding with `multi-qa-mpnet-base-dot-v1`
- ğŸ” Similarity search with `FAISS`
- ğŸš€ Web API via FastAPI
- ğŸ–¼ï¸ Streamlit UI for quick interaction
- ğŸ’¾ Local embedding cache
- ğŸ“œ Logging of all major components
- âœ… Unit tests with pytest

---

## ğŸ“ Project Structure

```
pdf-qa-app/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pdf_processor.py      # Extract text from PDFs
â”‚   â”œâ”€â”€ vector_store.py       # Embedding + FAISS storage + caching
â”‚   â”œâ”€â”€ llm_client.py         # LLaMA3 API wrapper via Ollama
â”‚   â”œâ”€â”€ config.py             # Configuration variables
â”‚   â””â”€â”€ api.py                # FastAPI endpoints (optional)
â”‚
â”œâ”€â”€ streamlit_app.py         # Streamlit UI entrypoint
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_pdf_processor.py
â”‚   â”œâ”€â”€ test_vector_store.py
â”‚   â”œâ”€â”€ test_llm_client.py
â”‚
â”œâ”€â”€ logs/                    # Logs saved locally
â”œâ”€â”€ vector_cache/           # Saved FAISS indexes and texts
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ main.py                  # Entrypoint (FastAPI or CLI)
```

---

## ğŸš€ Setup Instructions

### 1. Clone the repo
```bash
git clone https://github.com/yourname/pdf-qa-app.git
cd pdf-qa-app
```

### 2. Create and activate virtualenv
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Start Ollama (if not already)
```bash
ollama serve
ollama run llama3
```

### 5. Run the app
#### FastAPI
```bash
uvicorn main:app --reload --port 8000
```
#### Streamlit UI
```bash
streamlit run streamlit_app.py
```

---

## Run Tests

```bash
pytest tests/
```

## / Run the App
```bash
streamlit run streamlit_app/streamlit_app.py
```
---

## âš™ï¸ Configuration

Modify `app/config.py`:

```python
EMBEDDING_MODEL = "multi-qa-mpnet-base-dot-v1"
CHUNK_SIZE = 300
CHUNK_OVERLAP = 50
VECTOR_CACHE_DIR = "vector_cache"
```

---

## ğŸ“ˆ Logs

Logs are saved to the `logs/` directory per component:
- `pdf_processor.log`
- `vector_store.log`
- `llm_client.log`

---

## âœ… TODOs

- [x] Add Streamlit UI
- [ ] Enhance logging module
- [ ] Upload multiple PDFs
- [ ] Support multiple LLMs via config
- [ ] Dockerize the app
- [ ] Add authentication (optional)
- [ ] Switch from transformers pipeline to vLLM, Ollama, or LangChain for RAG enhancements

---

## ğŸ“œ License

MIT License Â© 2025 Li Ji
